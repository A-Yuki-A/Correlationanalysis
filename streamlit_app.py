# streamlit_app.py
# ã¨ã©ãƒ©ãƒ³ã®ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨˜äº‹URLã‚’2ã¤è²¼ã‚Šä»˜ã‘ã¦ã€
# ã€Œéƒ½é“åºœçœŒ Ã— å®Ÿæ•°å€¤ï¼ˆåå·®å€¤ã‚„é †ä½ã¯é™¤å¤–ï¼‰ã€ã‚’è‡ªå‹•æŠ½å‡ºã€‚
# è¡¨ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆ<caption>/<h1> ç­‰ï¼‰ã‚’ãƒ©ãƒ™ãƒ«ã«åæ˜ ã—ã€
# ç›¸é–¢ä¿‚æ•°ãƒ»æ±ºå®šä¿‚æ•°ãƒ»æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šã¤ãï¼‰ãƒ»ç®±ã²ã’å›³ã‚’è¡¨ç¤ºã€‚
# æ•£å¸ƒå›³ã¯å¹…640pxï¼ˆå‰ã‚ˆã‚Š2å€ï¼‰ã€ç®±ã²ã’å›³ã¯å·¦å³ã«æ¨ªä¸¦ã³ã€‚
# Matplotlibã¯ japanize-matplotlib ã§æ—¥æœ¬èªåŒ–ï¼ˆç„¡ã„ç’°å¢ƒã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ã€‚

import io
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import streamlit as st
import requests
from bs4 import BeautifulSoup
from pandas.api.types import is_scalar

# --- æ—¥æœ¬èªåŒ–ï¼ˆjapanize ãŒã‚ã‚Œã°ä½¿ã†ï¼‰ ---
try:
    import japanize_matplotlib  # noqa: F401
    plt.rcParams["axes.unicode_minus"] = False
except Exception:
    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šæ‰‹å…ƒã«ã‚ã‚‹æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è‡ªå‹•æ¤œå‡º
    def _set_jp_font_fallback():
        candidates = [
            "Yu Gothic", "Yu Gothic UI", "Noto Sans CJK JP", "Noto Sans JP",
            "IPAexGothic", "IPAGothic", "Hiragino Sans", "Meiryo",
        ]
        for name in candidates:
            try:
                prop = fm.FontProperties(family=name)
                fm.findfont(prop, fallback_to_default=False)
                plt.rcParams["font.family"] = name
                break
            except Exception:
                continue
        plt.rcParams["axes.unicode_minus"] = False
    _set_jp_font_fallback()

st.set_page_config(page_title="éƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿ ç›¸é–¢ãƒ„ãƒ¼ãƒ«ï¼ˆURLç‰ˆï¼‰", layout="wide")
st.title("éƒ½é“åºœçœŒãƒ‡ãƒ¼ã‚¿ ç›¸é–¢ãƒ„ãƒ¼ãƒ«ï¼ˆURLç‰ˆï¼‰")
st.write(
    "ã¨ã©ãƒ©ãƒ³ã® **å„ãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨˜äº‹ã®URL** ã‚’2ã¤è²¼ã‚Šä»˜ã‘ã¦ãã ã•ã„ã€‚"
    "è¡¨ã®ã€Œåå·®å€¤ã€ã€Œé †ä½ã€ã¯ä½¿ã‚ãšã€**ç·æ•°ï¼ˆä»¶æ•°ãƒ»äººæ•°ãƒ»é‡‘é¡ãªã©ã®å®Ÿæ•°å€¤ï¼‰**ã‚’è‡ªå‹•æŠ½å‡ºã—ã€"
    "ãƒšãƒ¼ã‚¸å†…ã® **è¡¨ã‚¿ã‚¤ãƒˆãƒ«** ã‚’ã‚°ãƒ©ãƒ•ã®ãƒ©ãƒ™ãƒ«ã«åæ˜ ã—ã¾ã™ã€‚"
)

# -------------------- ç”»åƒè¡¨ç¤ºã‚µã‚¤ã‚º --------------------
# Matplotlib æ—¢å®šæ›ç®—: 6.4inch * 100dpi = 640px
BASE_W_INCH, BASE_H_INCH = 6.4, 4.8
EXPORT_DPI = 200                 # PNGä¿å­˜æ™‚ã®DPIï¼ˆé«˜ç²¾ç´°ï¼‰
SCATTER_WIDTH_PX = 640           # æ•£å¸ƒå›³ã¯å‰ã®2å€ï¼ˆ640pxï¼‰
BOX_WIDTH_PX = 320               # ç®±ã²ã’å›³ã¯å·¦å³ã«æ¨ªä¸¦ã³ç”¨ï¼ˆå„320pxï¼‰

def show_fig(fig, width_px: int):
    """figã‚’PNGã«ã—ã¦ã€æŒ‡å®špxå¹…ã§ç¢ºå®Ÿã«è¡¨ç¤ºã€‚"""
    buf = io.BytesIO()
    fig.savefig(buf, format="png", dpi=EXPORT_DPI, bbox_inches="tight")
    buf.seek(0)
    st.image(buf, width=width_px)
    plt.close(fig)

# -------------------- 47éƒ½é“åºœçœŒ --------------------
PREFS = [
    "åŒ—æµ·é“","é’æ£®çœŒ","å²©æ‰‹çœŒ","å®®åŸçœŒ","ç§‹ç”°çœŒ","å±±å½¢çœŒ","ç¦å³¶çœŒ","èŒ¨åŸçœŒ","æ ƒæœ¨çœŒ","ç¾¤é¦¬çœŒ",
    "åŸ¼ç‰çœŒ","åƒè‘‰çœŒ","æ±äº¬éƒ½","ç¥å¥ˆå·çœŒ","æ–°æ½ŸçœŒ","å¯Œå±±çœŒ","çŸ³å·çœŒ","ç¦äº•çœŒ","å±±æ¢¨çœŒ","é•·é‡çœŒ",
    "å²é˜œçœŒ","é™å²¡çœŒ","æ„›çŸ¥çœŒ","ä¸‰é‡çœŒ","æ»‹è³€çœŒ","äº¬éƒ½åºœ","å¤§é˜ªåºœ","å…µåº«çœŒ","å¥ˆè‰¯çœŒ","å’Œæ­Œå±±çœŒ",
    "é³¥å–çœŒ","å³¶æ ¹çœŒ","å²¡å±±çœŒ","åºƒå³¶çœŒ","å±±å£çœŒ","å¾³å³¶çœŒ","é¦™å·çœŒ","æ„›åª›çœŒ","é«˜çŸ¥çœŒ","ç¦å²¡çœŒ",
    "ä½è³€çœŒ","é•·å´çœŒ","ç†Šæœ¬çœŒ","å¤§åˆ†çœŒ","å®®å´çœŒ","é¹¿å…å³¶çœŒ","æ²–ç¸„çœŒ"
]
PREF_SET = set(PREFS)

# -------------------- ç·æ•°ç³»ï¼é™¤å¤–ç³»ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ --------------------
TOTAL_KEYWORDS = [
    "ç·æ•°","åˆè¨ˆ","ä»¶æ•°","äººæ•°","äººå£","ä¸–å¸¯","æˆ¸æ•°","å°æ•°","åº—èˆ—æ•°","ç—…åºŠæ•°","æ–½è¨­æ•°",
    "é‡‘é¡","é¡","è²»ç”¨","æ”¯å‡º","åå…¥","è²©å£²é¡","ç”Ÿç”£é¡","ç”Ÿç”£é‡","é¢ç©","å»¶ã¹","å»¶","æ•°",
]
RATE_WORDS = [
    "ç‡","å‰²åˆ","æ¯”ç‡","ï¼…","ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ",
    "äººå½“ãŸã‚Š","ä¸€äººå½“ãŸã‚Š","äººå£å½“ãŸã‚Š","åƒäººå½“ãŸã‚Š","10ä¸‡äººå½“ãŸã‚Š","å½“ãŸã‚Š"
]
EXCLUDE_WORDS = ["é †ä½","åå·®å€¤"]

# -------------------- ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ --------------------
def to_number(x) -> float:
    """æ–‡å­—åˆ—ã‹ã‚‰æ•°å€¤ï¼ˆå°æ•°å«ã‚€ï¼‰ã‚’æŠœãå‡ºã—ã¦ float åŒ–ã€‚é…åˆ—/SeriesãŒæ¥ã¦ã‚‚å®‰å…¨ã€‚"""
    if not is_scalar(x):
        try:
            x = x.item()
        except Exception:
            return np.nan
    s = str(x).replace(",", "").replace("ã€€", " ").replace("ï¼…", "%").strip()
    m = re.search(r"-?\d+(?:\.\d+)?", s)
    if not m:
        return np.nan
    try:
        return float(m.group(0))
    except Exception:
        return np.nan

def draw_scatter_with_reg(df: pd.DataFrame, la: str, lb: str):
    """æ•£å¸ƒå›³ï¼‹å›å¸°ç›´ç·šï¼ˆæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼‰ã€‚"""
    x = pd.to_numeric(df["value_a"], errors="coerce")
    y = pd.to_numeric(df["value_b"], errors="coerce")
    mask = x.notna() & y.notna()
    x = x[mask].to_numpy()
    y = y[mask].to_numpy()

    fig, ax = plt.subplots(figsize=(BASE_W_INCH, BASE_H_INCH))
    ax.scatter(x, y)

    if len(x) >= 2:
        slope, intercept = np.polyfit(x, y, 1)
        xs = np.linspace(x.min(), x.max(), 200)
        ax.plot(xs, slope * xs + intercept, label=f"å›å¸°ç›´ç·š: y = {slope:.3g}x + {intercept:.3g}")
        r = np.corrcoef(x, y)[0, 1]
        ax.legend(loc="best", frameon=False, title=f"r = {r:.3f}, RÂ² = {r**2:.3f}")

    ax.set_xlabel(la)  # æ—¥æœ¬èª
    ax.set_ylabel(lb)  # æ—¥æœ¬èª
    ax.set_title("æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šã¤ãï¼‰")
    show_fig(fig, SCATTER_WIDTH_PX)

def draw_boxplot(series: pd.Series, label: str):
    """ç®±ã²ã’å›³ï¼ˆæ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼‰ã€‚"""
    fig, ax = plt.subplots(figsize=(BASE_W_INCH, BASE_H_INCH))
    ax.boxplot(pd.to_numeric(series, errors="coerce").dropna(), vert=True)
    ax.set_title(f"ç®±ã²ã’å›³ï¼š{label}")
    ax.set_ylabel("å€¤")
    ax.set_xticks([1])
    ax.set_xticklabels([label])  # æ—¥æœ¬èªãƒ©ãƒ™ãƒ«
    show_fig(fig, BOX_WIDTH_PX)

def flatten_columns(cols) -> list:
    if isinstance(cols, pd.MultiIndex):
        flat = []
        for tup in cols:
            parts = [str(x) for x in tup if pd.notna(x)]
            parts = [p for p in parts if not p.startswith("Unnamed")]
            name = " ".join(parts).strip()
            flat.append(name if name else "col")
        return flat
    return [str(c).strip() for c in cols]

def make_unique(seq: list) -> list:
    seen = {}
    out = []
    for c in seq:
        if c in seen:
            seen[c] += 1
            out.append(f"{c}__{seen[c]}")
        else:
            seen[c] = 1
            out.append(c)
    return out

def is_rank_like(nums: pd.Series) -> bool:
    s = pd.to_numeric(nums, errors="coerce").dropna()
    if s.empty:
        return False
    ints = (np.abs(s - np.round(s)) < 1e-9)
    share_int = float(ints.mean())
    in_range = float(((s >= 1) & (s <= 60)).mean())
    unique_close = (s.nunique() >= min(30, len(s)))
    return (share_int >= 0.8) and (in_range >= 0.9) and unique_close

def compose_label(caption: str | None, val_col: str | None, page_title: str | None) -> str:
    for s in (caption, page_title, val_col, "ãƒ‡ãƒ¼ã‚¿"):
        if s and str(s).strip():
            return str(s).strip()
    return "ãƒ‡ãƒ¼ã‚¿"

# -------------------- URL â†’ (DataFrame, ãƒ©ãƒ™ãƒ«) --------------------
@st.cache_data(show_spinner=False)
def load_todoran_table(url: str, version: int = 17):
    """
    ã¨ã©ãƒ©ãƒ³è¨˜äº‹URLã‹ã‚‰ã€
    - df: columns = ['pref','value']ï¼ˆéƒ½é“åºœçœŒã¨ç·æ•°ç³»ã®å®Ÿæ•°å€¤ï¼‰
    - label: ã‚°ãƒ©ãƒ•ã«ä½¿ã†æ—¥æœ¬èªãƒ©ãƒ™ãƒ«ï¼ˆcaption > h1/title > å€¤åˆ—åï¼‰
    ã‚’è¿”ã™ã€‚
    """
    headers = {"User-Agent": "Mozilla/5.0 (compatible; Streamlit/URL-extractor)"}
    r = requests.get(url, headers=headers, timeout=20)
    r.raise_for_status()
    html = r.text
    soup = BeautifulSoup(html, "lxml")

    page_h1 = soup.find("h1").get_text(strip=True) if soup.find("h1") else None
    page_title = soup.title.get_text(strip=True) if soup.title else None

    try:
        tables = pd.read_html(html, flavor="lxml")
    except Exception:
        try:
            tables = pd.read_html(html, flavor="bs4")
        except Exception:
            tables = []

    bs_tables = soup.find_all("table")

    def pick_value_dataframe(df: pd.DataFrame):
        df = df.copy()
        df.columns = make_unique(flatten_columns(df.columns))
        df = df.loc[:, ~df.columns.duplicated()]

        cols = list(df.columns)
        pref_cols = [c for c in cols if ("éƒ½é“åºœçœŒ" in c) or (c in ("çœŒå", "é“åºœçœŒ", "åºœçœŒ"))]
        if not pref_cols:
            return None, None

        def bad_name(name: str) -> bool:
            n = str(name)
            return any(w in n for w in EXCLUDE_WORDS)

        raw_value_candidates = [
            c for c in cols
            if (c not in ("é †ä½","éƒ½é“åºœçœŒ","é“åºœçœŒ","çœŒå","åºœçœŒ")) and (not bad_name(c))
        ]
        total_name_candidates = [
            c for c in raw_value_candidates
            if any(k in c for k in TOTAL_KEYWORDS) and not any(r in c for r in RATE_WORDS)
        ]
        fallback_candidates = [
            c for c in raw_value_candidates
            if not any(r in c for r in RATE_WORDS)
        ]

        def score_and_build(pref_col: str, candidate_cols: list):
            best_score = -1
            best_df = None
            best_vc = None

            pref_series = df[pref_col]
            if isinstance(pref_series, pd.DataFrame):
                pref_series = pref_series.iloc[:, 0]
            pref_series = pref_series.map(lambda x: str(x).strip())
            mask = pref_series.isin(PREF_SET).to_numpy()
            if not mask.any():
                return None, None

            for vc in candidate_cols:
                if vc not in df.columns:
                    continue
                col = df[vc]
                if isinstance(col, pd.DataFrame):
                    col = col.iloc[:, 0]
                col_num = pd.to_numeric(col.map(to_number), errors="coerce")
                col_num = col_num.loc[mask]

                if is_rank_like(col_num):
                    continue

                base = int(col_num.notna().sum())
                bonus = 15 if any(k in vc for k in TOTAL_KEYWORDS) else 0
                score = base + bonus

                if score > best_score and base >= 30:
                    tmp = pd.DataFrame({"pref": pref_series.loc[mask].values, "value": col_num.values})
                    tmp = tmp.dropna(subset=["value"]).drop_duplicates(subset=["pref"])
                    best_score = score
                    best_df = tmp
                    best_vc = vc

            return best_df, best_vc

        for pref_col in pref_cols:
            got, val_col = score_and_build(pref_col, total_name_candidates)
            if got is not None:
                got["pref"] = pd.Categorical(got["pref"], categories=PREFS, ordered=True)
                got = got.sort_values("pref").reset_index(drop=True)
                return got, val_col

        for pref_col in pref_cols:
            got, val_col = score_and_build(pref_col, fallback_candidates)
            if got is not None:
                got["pref"] = pd.Categorical(got["pref"], categories=PREFS, ordered=True)
                got = got.sort_values("pref").reset_index(drop=True)
                return got, val_col

        return None, None

    # read_html ã®å„è¡¨ã‚’è©¦ã—ã€caption ã‚’ãƒ©ãƒ™ãƒ«å€™è£œã«ä½¿ã†
    for idx, raw in enumerate(tables):
        got, val_col = pick_value_dataframe(raw)
        if got is not None:
            caption_text = None
            if idx < len(bs_tables):
                cap = bs_tables[idx].find("caption")
                if cap:
                    caption_text = cap.get_text(strip=True)
            label = compose_label(caption_text, val_col, page_h1 or page_title)
            return got, label

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆç°¡æ˜“æŠ½å‡ºï¼‰
    lines = []
    for tag in soup.find_all(text=True):
        t = str(tag).strip()
        if t:
            lines.append(t)
    text = "\n".join(lines)

    rows = []
    for line in text.splitlines():
        m = re.search(r"(åŒ—æµ·é“|..çœŒ|..åºœ|æ±äº¬éƒ½)\s+(-?\d+(?:\.\d+)?)", line)
        if m:
            pref = m.group(1)
            val = float(m.group(2))
            if pref in PREF_SET:
                rows.append((pref, val))

    if rows:
        # ğŸ‘‡ ã“ã“ã‚’ä¿®æ­£ï¼šä½™åˆ†ãª ']' ã‚’å‰Šé™¤
        work = pd.DataFrame(rows, columns=["pref", "value"]).drop_duplicates("pref")
        work["pref"] = pd.Categorical(work["pref"], categories=PREFS, ordered=True)
        work = work.sort_values("pref").reset_index(drop=True)
        label = compose_label(None, None, page_h1 or page_title)
        return work, label

    return pd.DataFrame(columns=["pref","value"]), "ãƒ‡ãƒ¼ã‚¿"

# -------------------- UI --------------------
c1, c2 = st.columns(2)
with c1:
    url_a = st.text_input("ãƒ‡ãƒ¼ã‚¿Aã®URLï¼ˆã¨ã©ãƒ©ãƒ³è¨˜äº‹ï¼‰", placeholder="https://todo-ran.com/t/kiji/XXXXX")
with c2:
    url_b = st.text_input("ãƒ‡ãƒ¼ã‚¿Bã®URLï¼ˆã¨ã©ãƒ©ãƒ³è¨˜äº‹ï¼‰", placeholder="https://todo-ran.com/t/kiji/YYYYY")

if st.button("ç›¸é–¢ã‚’è¨ˆç®—ãƒ»è¡¨ç¤ºã™ã‚‹", type="primary"):
    if not url_a or not url_b:
        st.error("2ã¤ã®URLã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    try:
        df_a, label_a = load_todoran_table(url_a)
        df_b, label_b = load_todoran_table(url_b)
    except requests.RequestException as e:
        st.error(f"ãƒšãƒ¼ã‚¸ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸï¼š{e}")
        st.stop()

    if df_a.empty or df_b.empty:
        st.error("è¡¨ã®æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸã€‚URLãŒãƒ©ãƒ³ã‚­ãƒ³ã‚°è¨˜äº‹ã§ã‚ã‚‹ã“ã¨ã€è¡¨ã«ã€éƒ½é“åºœçœŒã€åˆ—ãŒã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.stop()

    # å…±é€šéƒ½é“åºœçœŒã§çµåˆ
    df = pd.merge(
        df_a.rename(columns={"value": "value_a"}),
        df_b.rename(columns={"value": "value_b"}),
        on="pref",
        how="inner",
    )

    # è¡¨ç¤ºç”¨ï¼šåˆ—åã«ãƒ©ãƒ™ãƒ«ã‚’ä½¿ã†ï¼ˆå†…éƒ¨è¨ˆç®—ã¯ value_a/value_bï¼‰
    display_df = df.rename(columns={"value_a": label_a, "value_b": label_b})

    st.subheader("çµåˆå¾Œã®ãƒ‡ãƒ¼ã‚¿ï¼ˆå…±é€šã®éƒ½é“åºœçœŒã®ã¿ï¼‰")
    st.dataframe(display_df, use_container_width=True, hide_index=True)

    if len(df) < 3:
        st.warning("å…±é€šãƒ‡ãƒ¼ã‚¿ãŒå°‘ãªã„ãŸã‚ã€ç›¸é–¢ä¿‚æ•°ãŒä¸å®‰å®šã§ã™ã€‚åˆ¥ã®æŒ‡æ¨™ã§ãŠè©¦ã—ãã ã•ã„ã€‚")
        st.stop()

    # ç›¸é–¢ï¼ˆæŒ‡å®šã®è¡¨è¨˜ã§å‡ºåŠ›ï¼‰
    x = pd.to_numeric(df["value_a"], errors="coerce")
    y = pd.to_numeric(df["value_b"], errors="coerce")
    mask = x.notna() & y.notna()
    r = float(x[mask].corr(y[mask]))
    r2 = r ** 2

    st.subheader("ç›¸é–¢ã®çµæœ")
    st.markdown(f"**ç›¸é–¢ä¿‚æ•° r = {r:.4f}**")
    st.markdown(f"**æ±ºå®šä¿‚æ•° r2 = {r2:.4f}**")

    # æ•£å¸ƒå›³ï¼ˆå›å¸°ç›´ç·šã¤ãï¼šå¹…640pxï¼‰
    st.subheader("æ•£å¸ƒå›³")
    draw_scatter_with_reg(df, label_a, label_b)

    # ç®±ã²ã’å›³ï¼ˆå·¦å³ã«æ¨ªä¸¦ã³ï¼šå„320pxï¼‰
    st.subheader("ç®±ã²ã’å›³")
    col_left, col_right = st.columns(2)
    with col_left:
        draw_boxplot(df["value_a"], label_a)
    with col_right:
        draw_boxplot(df["value_b"], label_b)

    # CSVãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆå†…éƒ¨åã®ã¾ã¾ä¿å­˜ï¼šåˆ†æå‘ã‘ï¼‰
    csv = df.to_csv(index=False).encode("utf-8-sig")
    st.download_button(
        "çµåˆãƒ‡ãƒ¼ã‚¿ã‚’CSVã§ä¿å­˜ï¼ˆå†…éƒ¨åˆ—åï¼švalue_a/value_bï¼‰",
        data=csv,
        file_name="todoran_merged.csv",
        mime="text/csv",
    )
else:
    st.info("ä¸Šã®2ã¤ã®å…¥åŠ›æ¬„ã« ã¨ã©ãƒ©ãƒ³è¨˜äº‹ã®URL ã‚’è²¼ã£ã¦ã‹ã‚‰ã€Œç›¸é–¢ã‚’è¨ˆç®—ãƒ»è¡¨ç¤ºã™ã‚‹ã€ã‚’æŠ¼ã—ã¦ãã ã•ã„ã€‚")
